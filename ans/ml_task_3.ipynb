{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Interview challenge for PayPay.\n",
    "\n",
    "Data: markplace web session log on data folder\n",
    "\n",
    "Machine Learning Processing & Analytical goal:\n",
    "\n",
    "    - Predict the number of unique URL visits by a given IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Libraries and setup\n",
    "# Auto reload changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# source dependencies, log_file_schema & data handler\n",
    "from src.dependencies import *\n",
    "from src.log_file_schema import schema\n",
    "from src.data_handler import DataHandler\n",
    "\n",
    "# plot lib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# forcasting lib and matrics\n",
    "# https://xgboost.readthedocs.io/en/latest/\n",
    "import xgboost\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#create or get spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"PayPayChallenge\")\\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+------+------+------+------+\n|count_unique_URLs|octet0|octet1|octet2|octet3|\n+-----------------+------+------+------+------+\n|               14|    59|   160|   110|   163|\n|                6|    27|    63|   186|    72|\n|                7|   120|    61|    47|    36|\n|               85|   115|   112|   250|   108|\n|               16|    61|    16|   142|   162|\n|               10|   123|   136|   182|   137|\n|               16|   117|   205|    39|   248|\n|                9|   117|   247|   188|    13|\n|               84|   113|   193|   114|    25|\n|              108|    14|   139|    82|   134|\n|              110|   202|    53|    89|   132|\n|               34|   117|   241|   152|    20|\n|                3|   117|   207|    97|   173|\n|              112|    27|    34|   244|   251|\n|                2|   117|   203|   181|   144|\n|               88|   124|   125|    22|   218|\n|               16|   202|   174|    92|    10|\n|               37|   103|    42|    88|    34|\n|               55|   117|   225|   184|   240|\n|                7|   123|    63|   194|   202|\n+-----------------+------+------+------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "def duration(start, end):\n",
    "    \"\"\"\n",
    "    Retunr time duration in seconds\n",
    "\n",
    "    @param start: timestamp\n",
    "    @param end: timestamnp\n",
    "    \"\"\"\n",
    "    try:\n",
    "        num_of_seconds = (end - start).total_seconds()\n",
    "    except:\n",
    "        num_of_seconds = 0\n",
    "    return num_of_seconds\n",
    "\n",
    "get_duration = udf(duration, FloatType())\n",
    "\n",
    "def preprocess_data(spark):\n",
    "    \"\"\"\n",
    "    Processing the data \n",
    "    :param spark: spark session\n",
    "    :return processed data \n",
    "    \"\"\"\n",
    "    df = spark.read.csv(log_file, schema=schema, sep=\" \").repartition(num_partitions).cache()\n",
    "    split_client = split(df[\"client:port\"], \":\")\n",
    "    split_backend = split(df[\"backend:port\"], \":\")\n",
    "    split_request = split(df[\"request\"], \" \")\n",
    "\n",
    "    df=df.withColumn(\"ip\", split_client.getItem(0)) \\\n",
    "                .withColumn(\"client_port\", split_client.getItem(1)) \\\n",
    "                .withColumn(\"backend_ip\", split_backend.getItem(0)) \\\n",
    "                .withColumn(\"backend_port\", split_backend.getItem(1)) \\\n",
    "                .withColumn(\"request_action\", split_request.getItem(0)) \\\n",
    "                .withColumn(\"request_url\", split_request.getItem(1)) \\\n",
    "                .withColumn(\"request_protocol\", split_request.getItem(2)) \\\n",
    "                .withColumn(\"current_timestamp\", col(\"timestamp\").cast(\"timestamp\")) \\\n",
    "                .drop(\"client:port\",\"backend:port\",\"request\").cache()\n",
    "\n",
    "    df=df.select([\"ip\", \"request_url\"])\n",
    "    df=df.na.drop(subset=[\"request_url\"])\n",
    "    df=df.na.drop(subset=[\"ip\"])\n",
    "    \n",
    "    df = df.groupby(\"ip\").agg(countDistinct(\"request_url\").alias(\"count_unique_URLs\"))\n",
    "    df=df.na.drop(subset=[\"count_unique_URLs\"])\n",
    "    \n",
    "    splitt2=split(df[\"ip\"], \"\\\\.\")\n",
    "    df=df.withColumn(\"octet0\", splitt2.getItem(0))\n",
    "    df=df.withColumn(\"octet1\", splitt2.getItem(1))\n",
    "    df=df.withColumn(\"octet2\", splitt2.getItem(2))\n",
    "    df=df.withColumn(\"octet3\", splitt2.getItem(3))\n",
    "    \n",
    "    df=df.drop(\"ip\")\n",
    "    df=df.na.drop(subset=[\"octet0\"])\n",
    "    df=df.na.drop(subset=[\"octet1\"])\n",
    "    df=df.na.drop(subset=[\"octet2\"])\n",
    "    df=df.na.drop(subset=[\"octet3\"])\n",
    "    #print(df.dtypes);\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def data_loader(spark):\n",
    "    \"\"\"\n",
    "    Data loader\n",
    "    \n",
    "    @param spark: spark session\n",
    "    \"\"\"\n",
    "    dataset3 = preprocess_data(spark).cache()\n",
    "    dataset3.show()\n",
    "    return dataset3.select(\"*\").toPandas()\n",
    "\n",
    "df=data_loader(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  octet0 octet1 octet2 octet3  count_unique_URLs\n0     59    160    110    163                 14\n1     27     63    186     72                  6\n2    120     61     47     36                  7\n3    115    112    250    108                 85\n4     61     16    142    162                 16\n(90544, 5)\n"
     ]
    }
   ],
   "source": [
    "df=df[['octet0', 'octet1', 'octet2', 'octet3', 'count_unique_URLs']]\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE: 98.38995270596601\n"
     ]
    }
   ],
   "source": [
    "#  lib and matrics\n",
    "# https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "df = df.apply(pd.to_numeric)\n",
    "X = df[['octet0', 'octet1', 'octet2', 'octet3']]\n",
    "Y = df[['count_unique_URLs']]\n",
    "\n",
    "model = xgboost.XGBRegressor(objective='reg:squarederror')\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"RMSE:\", np.mean(np.sqrt(np.abs(results))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE Randomforest: 81.50367587573092\n"
     ]
    }
   ],
   "source": [
    "#  lib and matrics\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor()\n",
    "model_rf.fit(X, Y)\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model_rf, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"RMSE Randomforest:\", np.mean(np.sqrt(np.abs(results))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('ad-hzm-video')",
   "display_name": "Python 3.8.5 64-bit ('ad-hzm-video')",
   "metadata": {
    "interpreter": {
     "hash": "22aa93368867714963484c9d6f9b248e60873c4fbd9f31b8702991b4180e19bd"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}